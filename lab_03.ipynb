{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T13:31:06.170827300Z",
     "start_time": "2024-10-16T13:31:05.924622500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas wykonania wersji sekwencyjnej: 0.02 sekund\n",
      "Czas wykonania wersji zrównoleglonej: 0.21 sekund\n",
      "Różnica czasu (zrównoleglony - sekwencyjny): 0.19 sekund\n",
      "\n",
      "Sekwencyjny DataFrame:\n",
      "                 date level client hostname         process    pid  \\\n",
      "0 2024-06-14 15:16:01                 combo  sshd(pam_unix)  19939   \n",
      "1 2024-06-14 15:16:02                 combo  sshd(pam_unix)  19937   \n",
      "2 2024-06-14 15:16:02                 combo  sshd(pam_unix)  19937   \n",
      "3 2024-06-15 02:04:59                 combo  sshd(pam_unix)  20882   \n",
      "4 2024-06-15 02:04:59                 combo  sshd(pam_unix)  20884   \n",
      "\n",
      "                                             message  \n",
      "0  authentication failure; logname= uid=0 euid=0 ...  \n",
      "1                           check pass; user unknown  \n",
      "2  authentication failure; logname= uid=0 euid=0 ...  \n",
      "3  authentication failure; logname= uid=0 euid=0 ...  \n",
      "4  authentication failure; logname= uid=0 euid=0 ...  \n",
      "\n",
      "Zrównoleglony DataFrame:\n",
      "                 date level client hostname         process    pid  \\\n",
      "0 2024-06-14 15:16:01                 combo  sshd(pam_unix)  19939   \n",
      "1 2024-06-14 15:16:02                 combo  sshd(pam_unix)  19937   \n",
      "2 2024-06-14 15:16:02                 combo  sshd(pam_unix)  19937   \n",
      "3 2024-06-15 02:04:59                 combo  sshd(pam_unix)  20882   \n",
      "4 2024-06-15 02:04:59                 combo  sshd(pam_unix)  20884   \n",
      "\n",
      "                                             message  \n",
      "0  authentication failure; logname= uid=0 euid=0 ...  \n",
      "1                           check pass; user unknown  \n",
      "2  authentication failure; logname= uid=0 euid=0 ...  \n",
      "3  authentication failure; logname= uid=0 euid=0 ...  \n",
      "4  authentication failure; logname= uid=0 euid=0 ...  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Zadanie 1\n",
    "Jieming Zhu, Shilin He, Pinjia He, Jinyang Liu, Michael R. Lyu. Loghub: A Large Collection of System Log Datasets for AI-driven Log Analytics. IEEE International Symposium on Software Reliability Engineering (ISSRE), 2023.\n",
    "'''\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def parse(inp: str):\n",
    "    record = {}\n",
    "    \n",
    "    timestamp_str = inp[:15]\n",
    "    record[\"date\"] = timestamp_str.strip()\n",
    "    \n",
    "    rest = inp[16:].strip()\n",
    "    \n",
    "    hostname_end = rest.find(' ')\n",
    "    hostname = rest[:hostname_end]\n",
    "    rest = rest[hostname_end+1:]\n",
    "    \n",
    "    process_end = rest.find(':')\n",
    "    process_info = rest[:process_end]\n",
    "    message = rest[process_end+1:].strip()\n",
    "    \n",
    "    if '[' in process_info and ']' in process_info:\n",
    "        process_name = process_info[:process_info.find('[')]\n",
    "        pid = process_info[process_info.find('[')+1:process_info.find(']')]\n",
    "    else:\n",
    "        process_name = process_info\n",
    "        pid = ''\n",
    "    \n",
    "    record[\"level\"] = ''\n",
    "    record[\"client\"] = ''\n",
    "    record[\"hostname\"] = hostname\n",
    "    record[\"process\"] = process_name.strip()\n",
    "    record[\"pid\"] = pid\n",
    "    record[\"message\"] = message\n",
    "    \n",
    "    return record\n",
    "\n",
    "def convert_date(rec):\n",
    "    rec[\"date\"] = datetime.strptime(rec[\"date\"], \"%b %d %H:%M:%S\")\n",
    "    rec[\"date\"] = rec[\"date\"].replace(year=datetime.now().year)\n",
    "    return rec\n",
    "\n",
    "with open('Linux_2k.log', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "start_time_seq = time.time()\n",
    "\n",
    "output_seq = []\n",
    "\n",
    "for line in lines:\n",
    "    record = parse(line)\n",
    "    record = convert_date(record)\n",
    "    output_seq.append(record)\n",
    "    \n",
    "df_seq = pd.DataFrame(output_seq)\n",
    "\n",
    "df_seq.to_parquet('sekwencyjnie.parquet')\n",
    "\n",
    "end_time_seq = time.time()\n",
    "print(f\"Czas wykonania wersji sekwencyjnej: {end_time_seq - start_time_seq:.2f} sekund\")\n",
    "\n",
    "start_time_par = time.time()\n",
    "\n",
    "@delayed\n",
    "def process_line(line):\n",
    "    record = parse(line)\n",
    "    record = convert_date(record)\n",
    "    return record\n",
    "\n",
    "delayed_results = [process_line(line) for line in lines]\n",
    "\n",
    "results = dask.compute(*delayed_results)\n",
    "\n",
    "ddf = dd.from_pandas(pd.DataFrame(results), npartitions=4)\n",
    "\n",
    "ddf.to_parquet('zrownoleglone.parquet')\n",
    "\n",
    "end_time_par = time.time()\n",
    "print(f\"Czas wykonania wersji zrównoleglonej: {end_time_par - start_time_par:.2f} sekund\")\n",
    "\n",
    "print(f\"Różnica czasu (zrównoleglony - sekwencyjny): {(start_time_seq - end_time_seq) - (start_time_par - end_time_par):.2f} sekund\")\n",
    "\n",
    "print(\"\\nSekwencyjny DataFrame:\")\n",
    "print(df_seq.head())\n",
    "\n",
    "print(\"\\nZrównoleglony DataFrame:\")\n",
    "print(ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba rekordów z wygasłymi kartami kredytowymi: 19843\n",
      "Przykładowe rekordy z wygasłymi kartami:\n",
      "{'age': 111, 'name': ['Clemente', 'Carrillo'], 'occupation': 'Publishing Manager', 'telephone': '+16793348245', 'address': {'address': '1224 Valley Terrace', 'city': 'Sierra Vista'}, 'credit-card': {'number': '4103 2967 6457 4473', 'expiration-date': '02/25'}, 'ccexpires': '10/2021'}\n",
      "{'age': 83, 'name': ['Delbert', 'Kane'], 'occupation': 'Marine Engineer', 'telephone': '+1-627-346-6151', 'address': {'address': '1340 Lydia Center', 'city': 'Sandy Springs'}, 'credit-card': {'number': '3482 316212 35300', 'expiration-date': '03/24'}, 'ccexpires': '06/2024'}\n",
      "{'age': 64, 'name': ['Marget', 'Gould'], 'occupation': 'Circus Worker', 'telephone': '+16411791829', 'address': {'address': '1252 Olmstead Station', 'city': 'Garden Grove'}, 'credit-card': {'number': '5514 0498 5832 2386', 'expiration-date': '02/23'}, 'ccexpires': '01/2024'}\n"
     ]
    }
   ],
   "source": [
    "'''Zadanie 2'''\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "\n",
    "DATAPATH = './data'\n",
    "os.makedirs(DATAPATH, exist_ok=True)\n",
    "\n",
    "b = dask.datasets.make_people(npartitions=20, records_per_partition=5000)\n",
    "\n",
    "def modify_ccexpires(record):\n",
    "    if random.random() < 0.2:\n",
    "        past_date = datetime.now() - timedelta(days=random.randint(30, 365*5))\n",
    "        record['ccexpires'] = past_date.strftime('%m/%Y')\n",
    "    else:\n",
    "        future_date = datetime.now() + timedelta(days=random.randint(30, 365*5))\n",
    "        record['ccexpires'] = future_date.strftime('%m/%Y')\n",
    "    return record\n",
    "\n",
    "b = b.map(modify_ccexpires)\n",
    "\n",
    "b.map(json.dumps).to_textfiles(os.path.join(DATAPATH, 'people_modified_*.json'))\n",
    "\n",
    "b = db.read_text(os.path.join(DATAPATH, 'people_modified_*.json')).map(json.loads)\n",
    "\n",
    "def is_expired(record):\n",
    "    cc_expires = record.get('ccexpires', '')\n",
    "    if not cc_expires:\n",
    "        return False\n",
    "    try:\n",
    "        expiry_date = datetime.strptime(cc_expires, '%m/%Y')\n",
    "        last_day_of_month = expiry_date.replace(day=28) + timedelta(days=4)\n",
    "        last_day_of_month -= timedelta(days=last_day_of_month.day)\n",
    "        return last_day_of_month < datetime.now()\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "expired_cards = b.filter(is_expired)\n",
    "\n",
    "expired_cards = expired_cards.repartition(10)\n",
    "\n",
    "output_path = os.path.join(DATAPATH, 'expired_*.json')\n",
    "expired_cards.map(json.dumps).to_textfiles(output_path)\n",
    "\n",
    "expired_count = expired_cards.count().compute()\n",
    "print(f\"Liczba rekordów z wygasłymi kartami kredytowymi: {expired_count}\")\n",
    "\n",
    "print(\"Przykładowe rekordy z wygasłymi kartami:\")\n",
    "for record in expired_cards.take(3):\n",
    "    print(record)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T13:47:43.983260800Z",
     "start_time": "2024-10-16T13:47:37.944175500Z"
    }
   },
   "id": "d3ff3ad929cdbd5c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age                   name         occupation        telephone  \\\n",
      "0   64    ['Junko', 'Guzman']      Projectionist     +13148964143   \n",
      "1  114     ['Shona', 'Riggs']          Treasurer  +1-331-410-9646   \n",
      "2   65      ['Tommy', 'Best']  Transport Officer  +1-469-985-9150   \n",
      "3   47       ['Lino', 'Hays']      Audit Manager  +1-513-788-6110   \n",
      "4   44  ['Thurman', 'Hinton']            Student  +1-909-371-1490   \n",
      "\n",
      "                                             address  \\\n",
      "0  {'address': '579 Temescal Arcade', 'city': 'Po...   \n",
      "1  {'address': '644 Saint Louis Bayou', 'city': '...   \n",
      "2  {'address': '70 Garden Motorway', 'city': 'Sha...   \n",
      "3  {'address': '275 San Benito Freeway', 'city': ...   \n",
      "4  {'address': '1334 Pacific Circle', 'city': 'Bu...   \n",
      "\n",
      "                                         credit-card  \n",
      "0  {'number': '3791 977106 59878', 'expiration-da...  \n",
      "1  {'number': '3407 112039 63039', 'expiration-da...  \n",
      "2  {'number': '3711 870298 50084', 'expiration-da...  \n",
      "3  {'number': '2422 5120 7222 0254', 'expiration-...  \n",
      "4  {'number': '5462 8056 2327 8001', 'expiration-...  \n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "\n",
    "DATAPATH = './data'\n",
    "os.makedirs(DATAPATH, exist_ok=True)\n",
    "\n",
    "b = dask.datasets.make_people(npartitions=10, records_per_partition=1000)\n",
    "b.map(json.dumps).to_textfiles(os.path.join(DATAPATH, 'people_*.json'))\n",
    "\n",
    "b = db.read_text(os.path.join(DATAPATH, 'people_*.json')).map(json.loads)\n",
    "\n",
    "adults_bag = b.filter(lambda record: record['age'] >= 18)\n",
    "\n",
    "adults_df = adults_bag.to_dataframe()\n",
    "\n",
    "adults_df_pd = adults_df.compute()\n",
    "\n",
    "output_file = os.path.join(DATAPATH, 'adults.parquet')\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "adults_df_pd.to_parquet(output_file, index=False)\n",
    "\n",
    "df_parquet = pd.read_parquet(output_file)\n",
    "print(df_parquet.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T13:54:09.886053Z",
     "start_time": "2024-10-16T13:54:02.473239800Z"
    }
   },
   "id": "de638b0065b2394"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9627ee2f7037a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
